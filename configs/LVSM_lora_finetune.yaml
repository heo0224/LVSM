model:
  class_name: model.LVSM_scene_decoder_only.Images2LatentScene

  image_tokenizer:
    image_size: 256
    patch_size: 8
    in_channels: 9  # 3 RGB + 3 direction + 3 Reference

  target_pose_tokenizer:
    image_size: 256
    patch_size: 8
    in_channels: 6  # 3 direction + 3 Reference

  transformer:
    d: 768
    d_head: 64
    n_layer: 24
    special_init: true
    depth_init: true
    use_qk_norm: true

# LoRA Configuration
lora:
  enabled: true
  rank: 8                      # Low-rank dimension (4-32 typical)
  alpha: 16.0                  # Scaling factor (typically 2x rank)
  dropout: 0.05                # Dropout on LoRA path
  target_modules:              # Which layers to apply LoRA
    - "attn.to_qkv"            # Attention Q, K, V projection
    - "attn.fc"                # Attention output projection
    - "mlp.mlp.0"              # MLP up projection
    - "mlp.mlp.2"              # MLP down projection
  bias_trainable: false        # Keep biases frozen
  modules_to_save: []          # Additional modules to keep trainable

training:
  # LoRA training mode
  use_lora: true

  # Base model checkpoint to load before applying LoRA
  # Set this to your pretrained LVSM checkpoint path
  pretrained_checkpoint: ""

  # AMP settings
  amp_dtype: bf16
  use_amp: true
  use_tf32: true

  # Batch and data settings
  batch_size_per_gpu: 8
  num_input_views: 2
  num_target_views: 1
  num_views: 3
  target_has_input: false

  # Optimizer settings (higher LR typical for LoRA)
  lr: 0.001
  beta1: 0.9
  beta2: 0.95
  weight_decay: 0.0            # No weight decay for LoRA

  # Training steps
  train_steps: 50000
  warmup: 1000
  scheduler_type: cosine

  # Gradient settings
  grad_accum_steps: 1
  grad_checkpoint_every: 1
  grad_clip_norm: 1.0
  allowed_gradnorm_factor: 5

  # Checkpoint settings
  checkpoint_dir: ./ckpt/lora_finetune
  checkpoint_every: 1000
  save_lora_only: true         # Save only LoRA weights (smaller files)
  save_full_model_every: 10000 # Occasionally save full merged model

  # Dataset
  dataset_name: data.dataset_scene.Dataset
  dataset_path: ./preprocessed_data/train/full_list.txt

  # Loss weights
  l2_loss_weight: 1.0
  lpips_loss_weight: 0.0
  perceptual_loss_weight: 0.5

  # Image processing
  center_crop: true
  square_crop: true
  scene_scale_factor: 1.35

  # Data loading
  num_threads: 8
  num_workers: 4
  prefetch_factor: 32

  # View selection
  use_rel_pose: false
  view_selector:
    max_frame_dist: 192
    min_frame_dist: 25

  # Logging
  print_every: 20
  vis_every: 500
  wandb_log_every: 50
  wandb_exp_name: LVSM_lora_finetune
  wandb_project: LVSM

  # API keys (if needed)
  api_key_path: ./configs/api_keys.yaml

# Inference / evaluation
inference:
  if_inference: true
  compute_metrics: true
  merge_lora: true             # Merge LoRA weights for efficient inference
  view_idx_file_path: ./data/evaluation_index_re10k.json
  render_video: true
  render_video_config:
    traj_type: interpolate
    num_frames: 60
    loop_video: true
    order_poses: false
